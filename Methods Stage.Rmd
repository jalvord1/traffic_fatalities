---
title: "Methods Stage"
author: "Julianna Alvord, Tony Zhang, Kara VanAllen, Aurora Lopez, Vivian Wang"
date: "11/18/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(plyr)
library(knitr)
library(dplyr)
library(class)
library(mosaic)
library(tidyr)
library(maps)
library(ggplot2)
library(datasets)

#can change back to pattern = ".csv"
filenames <- list.files(path=getwd(), pattern = "accident.csv")

#reading in all of the DF's from the zip
for (i in filenames){  
   name <- gsub("-",".",i)
   name <- gsub(".csv","",name)  
   assign(name,read.csv(i))
}

#For use later?
#df_full <- join_all(list(accident, person, damage), by = 'ST_CASE', type = 'full')
```

#Cleaning the accident DF
```{r, results = "hide"}
accident1 <- accident %>%
  #creating percent_killed outcome variable
  mutate(total_persons = (PERNOTMVIT + PERMVIT),
         percent_killed = (FATALS/total_persons)) %>%
  #selecting columns we want to include in the models
  select(c(STATE, MONTH, DAY_WEEK, HOUR, MAN_COLL, TYP_INT, WRK_ZONE, 
           LGT_COND, WEATHER1, DRUNK_DR, percent_killed, total_persons))

med_percent_fatalities <- median(accident1$percent_killed)

accident1 <- accident1 %>%
  mutate(above_fatal_med = as.factor(ifelse(percent_killed >= med_percent_fatalities, "above", "below")))

#state
states <- c("Alabama", "Alaska", "Arizona", "Arkansas", "California", "Colorado", "Connecticut", "Delaware", 
            "District of Columbia", "Florida", "Georgia", "Hawaii", "Idaho", "Illinois", "Indiana", "Iowa", "Kansas", 
            "Kentucky", "Louisiana", "Maine", "Maryland", "Massachusetts", "Michigan", "Minnesota", "Mississippi", 
            "Missouri", "Montana", "Nebraska", "Nevada", "New Hampshire", "New Jersey", "New Mexico", "New York",
            "North Carolina", "North Dakota", "Ohio", "Oklahoma", "Oregon", "Pennsylvania", "Puerto Rico", "Rhode Island", 
            "South Carolina", "South Dakota", "Tennessee", "Texas", "Utah", "Vermont", "Virginia", "Virgin Islands", 
            "Washington", "West Virginia", "Wisconsin", "Wyoming")

accident1 <- accident1 %>%
  mutate(states_cat = factor(STATE, levels = c(1, 2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20,
                                               21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
                                               36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                                               52, 53, 54, 55, 56), labels = states)) %>%
  select(-STATE)

#month
months <- c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October",
            "November", "December")

accident1 <- accident1 %>%
  mutate(months_cat = factor(MONTH, levels = c(1:12), labels = months)) %>%
  select(-MONTH)

#day_week
days <- c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")

accident1 <- accident1 %>%
  mutate(day_week_cat = factor(DAY_WEEK, levels = c(1:7), labels = days)) %>%
  select(-DAY_WEEK)

#hour
accident1 <- accident1 %>%
  filter(!HOUR == 99) %>%
  mutate(hour_cat = as.factor(HOUR)) %>%
  select(-HOUR)

#manner of collision
collision <- c("not collision", "front-to-rear", "front-to-front", "angle", "sideswipe-same", "sideswipe-different",
               "read-to-side", "rear-to-rear", "other")

accident1 <- accident1 %>%
  filter(!MAN_COLL %in% c(98, 99)) %>%
  mutate(man_coll_cat = factor(MAN_COLL, levels = c(0,1,2,6,7,8,9,10,11), labels = collision)) %>%
  select(-MAN_COLL)

#type of intersection
intersection <- c("not intersection", "four-way", "t-intersection", "y-intersection", "traffic circle", "roundabout",
                  "five point+")

accident1 <- accident1 %>%
  filter(!TYP_INT %in% c(10, 98, 99)) %>%
  mutate(type_int_cat = factor(TYP_INT, levels = c(1:7), labels = intersection)) %>%
  select(-TYP_INT)

#work zone
accident1 <- accident1 %>%
  mutate(work_zone_bin = ifelse(WRK_ZONE == 0, 0, 1)) %>%
  select(-WRK_ZONE)

#light condition
light <- c("daylight", "dark-not lighted", "dark-lighted", "dawn", "dusk", "dark-unknown", "other")

accident1 <- accident1 %>%
  filter(!LGT_COND %in% c(8, 9)) %>%
  mutate(lgt_cond_cat = factor(LGT_COND, levels = c(1:7), labels = light)) %>%
  select(-LGT_COND)

#weather
weather <- c("none", "clear", "rain", "sleet", "snow", "fog", "crosswinds", "blowing sand", "other", "cloudy", 
             "blowing snow")

accident1 <- accident1 %>%
  filter(!WEATHER1 %in% c(98, 99)) %>%
  mutate(weather_cat = factor(WEATHER1, levels = c(1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12), labels = weather)) %>%
  select(-WEATHER1)

#drunk_driver
accident1 <- accident1 %>%
  mutate(drunk_dr_bin = ifelse(DRUNK_DR == 0, 0, 1)) %>%
  select(-DRUNK_DR)
```


#Images
```{r, results = "hide"}
#accidents per month
month <- accident %>%
  select(c(MONTH, ST_CASE)) %>%
  group_by(MONTH) %>%
  summarise(ftlmonth = n_distinct(ST_CASE))

ggplot(month, aes(x=MONTH, y=ftlmonth)) +
  geom_bar(stat = "identity")

#second image
## State group by
states_map <- map_data("state")
ggplot(states_map, aes(x = long, y = lat, group = group)) +   geom_polygon(fill = "white", color = "black")
accident <- accident %>%
  mutate(total_persons = (PERNOTMVIT + PERMVIT),
         percent_killed = (FATALS/total_persons))

accident_state_death <- accident %>% group_by(STATE) %>% summarise(total_death = sum(FATALS, na.rm = T))

# accident_state_death <- left_join(accident_state_death, state_name, by = "STATE")
# 
# names(accident_state_death)[3] <- "region"
# 
# state_map_death <- map_data("state") %>% left_join(accident_state_death, by = "region")
# 
# state_map_death <- arrange(state_map_death, group, order)

## First map for death
# ggplot(state_map_death, aes(x = long, y = lat, group = group, fill = total_death)) + geom_polygon(color = "black") +   coord_map("polyconic") +   scale_fill_gradient(low = "blue", high = "red")
 

#third image
accident_state_fatality <- accident %>% group_by(STATE) %>% summarise(percent_killed = sum(FATALS, na.rm = T) / sum(total_persons, na.rm = T))
# accident_state_fatality <- left_join(accident_state_fatality, state_name, by = "STATE")
# names(accident_state_fatality)[3] <- "region"
## Second map for fata
# state_map_fata <- map_data("state") %>% left_join(accident_state_fatality, by = "region")

# state_map_fata <- arrange(state_map_fata, group, order)
# ggplot(state_map_fata, aes(x = long, y = lat, group = group, fill = percent_killed)) + geom_polygon(color = "black") +   coord_map("polyconic") 

#fourth image

```

In our first image, we created a bar graph that counts the number of fatalities per month. As seen below in the numerical summary, we can see that February has the lowest number of fatalities while October has the most. 
![fatalities per month](month.jpg)

In our second image, we mapped the number of fatalities per state.
![fatalities per state](dt.png)

In our third image, we mapped the percent of people killed per state. This is our response variable.
![percent killed per state](Rplot.png)

#Numerical Summaries
```{r, results = "hide"}
#number of deaths per crash
favstats(~FATALS, data = accident)

#number of persons involved
favstats(~total_persons, data = accident1)

#cars involved
favstats(~VE_FORMS, data = accident)

#number of pedestrians involved
favstats(~PERNOTMVIT, data = accident)

#accidents per month
month <- accident %>%
  select(c(MONTH, ST_CASE)) %>%
  group_by(MONTH) %>%
  summarise(ftlmonth = n_distinct(ST_CASE)) %>%
  mutate(mean = mean(as.numeric(ftlmonth)))

kable(month)
```
The first numerical summary of the data calculates the summary statistics on the FATALS column, which is the total number of fatalities per crash. The range is (1,10) with 1.091 as the mean and 1 as the median.

The next numerical summary is of the total number of persons involved in each crash which is a sum of the persons in motor vehicle column and persons not in motor vehicle column. The range is (1,93) with 2.505 as the mean and 2 as the median.

The third numerical summary is for the number of cars involved in the fatal crash, per the VE_forms column. The range is (1,58) with 1.521 as the mean and 1 as the median.

The fourth numerical summary is the number of pedestrians involved in the crash with the range being (0,16) with 0.228 as the mean and zero as the median.

The fifth and last numerical summary is of the number of fatalities per month. This summary is not represented through favstats, but can be seen by the “kable” function. The month with the lowest number of fatalities has 1,968 (February) and the month with the most is October, with 3,019 fatalities.  The average number of fatalities is 2680.5 per month.

#Running Models

##QDA and LDA

As part of our predictions of whether a given incident led to a percentage killed that was above or below median, we decided to try LDA and QDA analysis.

To start, we considered the assumptions.

LDA makes a number of assumptions. The first is that the predictor variables X are drawn from a normal distribution. It also assumes that covariances are equal among the predictor variables X across all levels of Y. The number of predictor variables (p) must also be less than the sample size (n).

Compared to LDA, QDA is a more flexible classifier but does share some assumptions with LDA. It also assumes that the predictor variables are selected from a normal distribution. It, however, does not assume that the covariances are equal among the predictor variables X across all levels of Y. It also requires that the number of predictor variables is less than the sample size. With both LDA and QDA, the performance of the model will steeply decline as $p$ approaches $n$.

When considering which of the 54 predictors we wanted to use in our prediction of whether an accident would have a percentage of fatalities that was above or below the median, we realized that the variables that would be relevant within the context were largely categorical. Predictors such as state, month, day of the week, type of intersection, weather, and light condition, which intuitively seemed to be relevant in predicting the fatalities in an accident, were all categorical. However, since discriminant analysis assumes a multivariate normal distribution, categorical variables that are to be treated as predictors aren't handled well and violate this assumption rather strongly. Logistic regression presents a better approach because it makes no distributional assumptions of any kind. It's a direct probability model and doesn't require us to use Bayes' rule to convert results to probabilities as discriminant analysis does.

As a team, we've brainstormed two possible workarounds. The first would be to create dummy variables for each categorical variable. The second would be to try optimal scaling/quantification. The first would be incredibly tedious as we have quite a few categorical variables, each with numerous categories. The second we are unfamiliar with but are we willing to try if logistic regression proves unfruitful.

##KNN

We then decided to look into k-nearest neighbors. However, for the same reasons as above, we are unable to run this model. KNN requires continuous, numerical variables, and we have mostly categorical variables. Though kNN does not make assumptions about the underlying data distribution, kNN cannot work with categorical variables because there is no sense of "nearest" unless the variables are numerical. Specifically, kNN calculates Euclidean distance between points, which requires numbers, not categories.

Though we attempted to run code, as seen below, there were multiple errors. First, standardization is not possible for the same reason as stated in the previous paragraph. Second, the actual kNN function will not run because it is attempting to coerce the values of our variables into numbers, which is not possible.

```{r, results = "hide"}
accident1 <- accident1 %>%
  select(-c(total_persons,percent_killed))
#Standardizing the variables so they can be on comparable scale for "nearest" aspect of KNN
# standardized_accident1 <- accident1 %>%
#   select(-above_fatal_med) %>%
#   scale() %>%
#   data.frame()

#we can't run this above because of the categorical variables

#Test set of first 100 rows
test_accident = accident1 %>%
  slice(1:2000)

#will need to do cross validation

#Training data of remaining rows
train_accident = accident1 %>%
  slice(2001:31454)

#Dataframe containing only outcome variable
above_fatal_med <- accident1 %>%
  select(above_fatal_med)

#Creating vector of test data outcome values
test_above_fatal_med <- above_fatal_med %>%
  slice(1:2000) %>%
  .$above_fatal_med

#Creating vector of training data outcome values
train_above_fatal_med <- above_fatal_med %>%
  slice(2001:31454) %>%
  .$above_fatal_med

#base error rate
mean(test_above_fatal_med != "below") #68% of crashes above

#running the model
set.seed(1)
# knn_pred <- knn(train_accident, test_accident, train_above_fatal_med, k = 2)

#getting an error here- because of the categorical variables

#mean(test_above_fatal_med != knn_pred)
```


##Logistic Regression

Because the three other classification models we learned about will not run with categorical predictors, we finally decided to fit a logistic regression model. 

Thinking about assumptions: first, logistic regression requires observations to be independent of each other. In this case, each observation is an individual crash so likely these observations are in fact independent. Second, logistic regression requires little to no multicollinearity, which will be true in this case because we have almost entirely categorical variables. Third, there must be linearity between our predictors and the log odds. We assume this to be true, but this assumption may require more investigation. Finally, there must be a large enough sample size, which is true in this case. 

When running the code, we ran into an error with the predict function. The error we kept running into mentioned that the variable "state" in the test dataset includes new factor levels. Though we do not know how to fix this currently, we will likely use logistic regression as one of our classification models, given the amount of categorical values in our data.
```{r, results = "hide"}
accident1 <- accident1 %>%
  select(-c(total_persons,percent_killed))

accident1 <- accident1 %>%
  filter(!states_cat %in% c("District of Columbia", "Puerto Rico"),
         !man_coll_cat == "rear-to-rear",
         !lgt_cond_cat == "other")

accident1$id <- 1:nrow(accident1)
train <- accident1 %>% 
  dplyr::sample_frac(.1)

test  <- dplyr::anti_join(accident1, train, by = 'id')

# train <- accident1 %>%
#   slice(1:2000)
# 
# test <- accident1 %>%
#   slice(2001:31454)

glm_fit = glm(above_fatal_med~.,
              data = train,
              family = binomial)

summary(glm_fit)

glm_probs = data.frame(probs = predict(glm_fit,
                                       newdata = test,
                                       type="response"))
```



##Cross Validation

We are planning on running a k-fold cross-validation in this project. Within the code, we fit a logisitic regression model therefore we got the same error as above. Once we figure out the error with predicting using a glm, we will be able to run this cross-val.

#Conclusion

The main lesson we learned from starting our methods section is that classification models become much more complicated when there are categorical variables in the data. We look forward to finding out how to alter our data (or find different models) so that we may use different classification models and therefore predict more successfully. 

We look forward to figuring our optimate scaling and quantification for our variables in order to run QDA, LDA, and kNN. There is a lot for us to learn about these models when working with categorical data. Also, we need to fix the error in our logistic regression model code in order to get an idea of how successful we are at predicting.

Lastly, we will likely bring in other datasets that are available to us and join them to the dataset we are using here. This may offer us more predictors and possibly more numerical predictors. Our models, once we learn how to run them, may be more successful at predicting with these new varaibles.

Learning how to fit the models with categorical data and choosing new variables are the two aspects of this project that are most exciting to us, as we will sharply increase our knowledge of machine learning. 